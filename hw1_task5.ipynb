{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "axlVgXrQqf56"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import csv\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание 5"
      ],
      "metadata": {
        "id": "_2o6t503lTe7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_grouped_data(filename, groups_pattern, extra_columns=0, shuffle_rows=False):\n",
        "    rows = []\n",
        "    global_group_index = 0\n",
        "    for num_groups, records_per_group in groups_pattern:\n",
        "        for group in range(num_groups):\n",
        "            key = f\"grp{global_group_index}\"\n",
        "            for _ in range(records_per_group):\n",
        "                # формируем строку, начиная с ключа\n",
        "                line = key\n",
        "                # добавляем дополнительные колонки\n",
        "                for col in range(extra_columns):\n",
        "                    line += f\",val{col}\"\n",
        "                rows.append(line)\n",
        "            global_group_index += 1\n",
        "\n",
        "    if shuffle_rows:\n",
        "        random.shuffle(rows)\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        for line in rows:\n",
        "            f.write(line + \"\\n\")"
      ],
      "metadata": {
        "id": "6DKGQdv5zFfn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_key_counts(file_path, sample_prob, filter_keys=None):\n",
        "    # выбирает каждую строку с определенной вероятностью sample_prob\n",
        "    # подсчитывает вхождения ключей\n",
        "    counts = Counter()\n",
        "    sampled_total = 0\n",
        "    with open(file_path, 'r', newline='') as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            if random.random() < sample_prob:\n",
        "                sampled_total += 1\n",
        "                key = row[0]\n",
        "                if filter_keys is None or key in filter_keys:\n",
        "                    counts[key] += 1\n",
        "    return counts, sampled_total\n",
        "\n",
        "def exact_key_counts(file_path, candidate_keys):\n",
        "    # точный подсчет вхождений для ключей из candidate_keys\n",
        "    counts = Counter({k: 0 for k in candidate_keys})\n",
        "    with open(file_path, 'r', newline='') as f:\n",
        "        reader = csv.reader(f)\n",
        "        for row in reader:\n",
        "            key = row[0]\n",
        "            if key in counts:\n",
        "                counts[key] += 1\n",
        "    return counts\n",
        "\n",
        "def detect_heavy_keys(file1, file2, threshold, sample_prob=0.001):\n",
        "    # определяет проблемные ключи, которые встречаются не менее чем threshold раз в обоих файлах\n",
        "    # 1 – выборка по file1\n",
        "    # 2 – отбираем кандидатов, чья выборочная частота не меньше threshold * sample_prob / factor (ниже расписал подробнее об этом)\n",
        "    # 3 – аналогичная выборка по file2 для кандидатов\n",
        "    # 4 – для оставшихся кандидатов выполняем точный подсчет в обоих файлах и возвращаем те, у которых точная частота >= threshold\n",
        "\n",
        "    factor = 2\n",
        "    # factor нужен, чтобы компенсировать дисперсию выборки\n",
        "    # в случае, если мы пропустим частые ключи из-за недостаточной выборки, они все равно туда попадут\n",
        "    # при этом уменьшаем объем обрабатываемых данных за счет случайной выборки\n",
        "    expected_sample_threshold = threshold * sample_prob\n",
        "    # по expected_sample_threshold будем отбирать частые ключи\n",
        "    # threshold умножаем на sample_prob, т.к. выборка состоит из ключей, попавших с вероятностью sample_prob\n",
        "\n",
        "    # 1 – выборка по первому файлу\n",
        "    sample_counts1, sample_size1 = sample_key_counts(file1, sample_prob)\n",
        "\n",
        "    # 2 – отбираем кандидатов\n",
        "    candidate_set = {key for key, cnt in sample_counts1.items() if cnt >= expected_sample_threshold / factor}\n",
        "    if not candidate_set:\n",
        "        return []\n",
        "\n",
        "    # 3 – выборка по второму файлу только для кандидатов\n",
        "    sample_counts2, sample_size2 = sample_key_counts(file2, sample_prob, filter_keys=candidate_set)\n",
        "    # обновляем уже имеющийся набор кандидатов с условием на второй файл\n",
        "    candidate_set = {key for key in candidate_set if sample_counts2.get(key, 0) >= expected_sample_threshold / factor}\n",
        "    if not candidate_set:\n",
        "        return []\n",
        "\n",
        "    # 4 – точный подсчет в обоих файлах для оставшихся кандидатов\n",
        "    exact_counts1 = exact_key_counts(file1, candidate_set)\n",
        "    exact_counts2 = exact_key_counts(file2, candidate_set)\n",
        "    heavy_keys = [key for key in candidate_set\n",
        "                  if exact_counts1.get(key, 0) >= threshold and exact_counts2.get(key, 0) >= threshold]\n",
        "    return heavy_keys"
      ],
      "metadata": {
        "id": "khttaNpyfYiY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [(5, 80000), (50, 20000)]\n",
        "create_grouped_data(\"test_file1.csv\", pattern, extra_columns=0, shuffle_rows=True)\n",
        "create_grouped_data(\"test_file2.csv\", pattern, extra_columns=0, shuffle_rows=True)"
      ],
      "metadata": {
        "id": "FN6zIxoN46Db"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1_path = '/content/test_file1.csv'\n",
        "file2_path = '/content/test_file2.csv'\n",
        "heavy_threshold = 60000  # пороговое значение для проблемных ключей\n",
        "\n",
        "# sample_prob можно настроить: чем ниже, тем меньше данных в памяти, но выше дисперсия.\n",
        "problematic_keys = detect_heavy_keys(file1_path, file2_path, heavy_threshold, sample_prob=0.001)\n",
        "\n",
        "print(f\"Проблемные ключи (>= {heavy_threshold} в обоих файлах):\")\n",
        "for key in problematic_keys:\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7jP-o_K4wlF",
        "outputId": "d1c7aa2e-67ad-467c-d177-b0e6da87018b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проблемные ключи (>= 60000 в обоих файлах):\n",
            "grp3\n",
            "grp0\n",
            "grp1\n",
            "grp2\n",
            "grp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = [(1, 61000), (99, 20000)]\n",
        "create_grouped_data(\"test_file1.csv\", pattern, extra_columns=0, shuffle_rows=True)\n",
        "create_grouped_data(\"test_file2.csv\", pattern, extra_columns=0, shuffle_rows=True)"
      ],
      "metadata": {
        "id": "B2vK4y036cHN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1_path = '/content/test_file1.csv'\n",
        "file2_path = '/content/test_file2.csv'\n",
        "heavy_threshold = 60000  # пороговое значение для проблемных ключей\n",
        "\n",
        "# sample_prob можно настроить: чем ниже, тем меньше данных в памяти, но выше дисперсия.\n",
        "problematic_keys = detect_heavy_keys(file1_path, file2_path, heavy_threshold, sample_prob=0.001)\n",
        "\n",
        "print(f\"Проблемные ключи (>= {heavy_threshold} в обоих файлах):\")\n",
        "for key in problematic_keys:\n",
        "    print(key)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pXeyRRp6cJ6",
        "outputId": "99464800-356f-43ac-eb7a-c0207ff5e27b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Проблемные ключи (>= 60000 в обоих файлах):\n",
            "grp0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}